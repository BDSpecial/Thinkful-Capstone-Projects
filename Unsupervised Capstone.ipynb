{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Capstone: Classifying Authors\n",
    "### By Carley Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Unsupervised Capstone project is working to classify authors by their texts using natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import re\n",
    "import csv\n",
    "from scipy import stats, integrate\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "For this capstone I picking a set of texts from 10 different authors that had a least 100 datapoints for each author. I chose 10 authors from the Gutenberg corpus. Each of these authors is a well know classic and their books were either novels or poetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the books and reading in the data, this time in the form of paragraphs\n",
    "austen = gutenberg.paras('austen-persuasion.txt')\n",
    "blake = gutenberg.paras('blake-poems.txt')\n",
    "bryant = gutenberg.paras('bryant-stories.txt')\n",
    "burgess = gutenberg.paras('burgess-busterbrown.txt')\n",
    "carroll = gutenberg.paras('carroll-alice.txt')\n",
    "chesterton = gutenberg.paras('chesterton-ball.txt')\n",
    "edgeworth = gutenberg.paras('edgeworth-parents.txt')\n",
    "melville = gutenberg.paras('melville-moby_dick.txt')\n",
    "milton = gutenberg.paras('milton-paradise.txt')\n",
    "whitman = gutenberg.paras('whitman-leaves.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = [edgeworth, melville, whitman, chesterton, bryant, austen, carroll, blake, burgess, milton]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dataframe\n",
    "\n",
    "Here I am going to separate each text into paragraphs for each author. This will then allow me to perform the classification and natural language processing on these individual paragraphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for the paragraph texts and authors\n",
    "paragraph_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing\n",
    "paragraph_list = []\n",
    "author_list = []\n",
    "\n",
    "for book in book_list:\n",
    "    # Loop through each paragraph\n",
    "    for paragraphs in book:\n",
    "        # If the paragraph has only one sentence then it is likely a Chapter or Title\n",
    "        if len(paragraphs) == 1:\n",
    "            # Loop through the words of this single sentence paragraph\n",
    "            for words in paragraphs:\n",
    "                # Only use it if there are more than 2 words in it. \n",
    "                if len(words) > 2:\n",
    "                    para=paragraphs[0]\n",
    "                    # removing the double-dash from all words\n",
    "                    para=[re.sub(r'--','',word) for word in para]\n",
    "                    # Forming each paragraph into a string and adding it to the list of strings.\n",
    "                    paragraph_list.append(' '.join(para))\n",
    "                    # Each time a paragraph is appended, add the corresponding author to another list\n",
    "                    author_list.append('{}'.format(book[0][0]))\n",
    "        # Carry out the same process for all paragraphs that have more than one sentence. \n",
    "        else:\n",
    "            para=paragraphs[0]\n",
    "            #removing the double-dash from all words\n",
    "            para=[re.sub(r'--','',word) for word in para]\n",
    "            #Forming each paragraph into a string and adding it to the list of strings.\n",
    "            paragraph_list.append(' '.join(para))\n",
    "            author_list.append('{}'.format(book[0][0]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now clean up the paragraphs in the lists\n",
    "clean_list = []\n",
    "for each in paragraph_list:\n",
    "    each = re.sub(r'\\s+([\\'-;,?.!\"])', r'\\1', each)\n",
    "    each = \"\".join(each.split(\"} \"))\n",
    "    each = \"'\".join(each.split(\"\\' \"))\n",
    "    each = \"-\".join(each.split(\"- \"))\n",
    "    clean_list.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these clean paragraphs and authors to the dataframe\n",
    "paragraph_df['paragraphs'] = clean_list\n",
    "paragraph_df['authors'] = author_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a column with an encoded label for each author\n",
    "lb_make = LabelEncoder()\n",
    "paragraph_df['author_codes'] = lb_make.fit_transform(paragraph_df['authors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ The Parent's Assistant, by Maria Edgeworth ]</td>\n",
       "      <td>['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE ORPHANS.</td>\n",
       "      <td>['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Near the ruins of the castle of Rossmore, in I...</td>\n",
       "      <td>['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mary was at this time about twelve years old.</td>\n",
       "      <td>['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" No need to stop the wheel, Mary, dear, for m...</td>\n",
       "      <td>['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paragraphs  \\\n",
       "0     [ The Parent's Assistant, by Maria Edgeworth ]   \n",
       "1                                       THE ORPHANS.   \n",
       "2  Near the ruins of the castle of Rossmore, in I...   \n",
       "3      Mary was at this time about twelve years old.   \n",
       "4  \" No need to stop the wheel, Mary, dear, for m...   \n",
       "\n",
       "                                             authors  author_codes  \n",
       "0  ['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...             9  \n",
       "1  ['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...             9  \n",
       "2  ['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...             9  \n",
       "3  ['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...             9  \n",
       "4  ['[', 'The', 'Parent', \"'\", 's', 'Assistant', ...             9  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Completed Dataframe\n",
    "paragraph_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Author Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an dataframe for the analyzing the authors\n",
    "author_count_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Column with each author listed\n",
    "author_count_df['authors'] = ['edgeworth', 'melville', 'whitman', 'chesterton', 'bryant', 'austen', 'carroll', 'blake', 'burgess', 'milton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function to cound the average length of each book's paragraphs\n",
    "def average_paragraph_length(book):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for paragraphs in book:\n",
    "        #If the length of the paragraph is one then it is likely a Chapter or Title\n",
    "        if len(paragraphs) == 1:\n",
    "            for words in paragraphs:\n",
    "                # Only use it if there are more than 2 words in it. \n",
    "                if len(words) > 2:\n",
    "                    # Add the length of this paragraph to the total\n",
    "                    total += len(paragraphs)\n",
    "                    # Increase the count by one. \n",
    "                    count +=1\n",
    "        else:\n",
    "            # Add the length of this paragraph to the total\n",
    "            total += len(paragraphs)\n",
    "            # Increase the count by one. \n",
    "            count +=1\n",
    "\n",
    "    return round(total/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_length = []\n",
    "for each in book_list:\n",
    "    # Use this function on each book and append it to a list\n",
    "    paragraph_length.append(average_paragraph_length(each))\n",
    "\n",
    "# Create a new column for each author's average paragraph length\n",
    "author_count_df['paragraph_length'] = paragraph_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Parent', \"'\", 's', 'Assistant', ',', 'by', 'Maria', 'Edgeworth', ']']                       3721\n",
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']']                                            2636\n",
      "['[', 'Leaves', 'of', 'Grass', 'by', 'Walt', 'Whitman', '1855', ']']                                      2436\n",
      "['[', 'The', 'Ball', 'and', 'The', 'Cross', 'by', 'G', '.', 'K', '.', 'Chesterton', '1909', ']']          1605\n",
      "['[', 'Stories', 'to', 'Tell', 'to', 'Children', 'by', 'Sara', 'Cone', 'Bryant', '1918', ']']             1186\n",
      "['[', 'Persuasion', 'by', 'Jane', 'Austen', '1818', ']']                                                  1007\n",
      "['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']']          814\n",
      "['[', 'Poems', 'by', 'William', 'Blake', '1789', ']']                                                      255\n",
      "['[', 'The', 'Adventures', 'of', 'Buster', 'Bear', 'by', 'Thornton', 'W', '.', 'Burgess', '1920', ']']     243\n",
      "['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', '1667', ']']                                              17\n",
      "Name: authors, dtype: int64\n",
      "9    3721\n",
      "2    2636\n",
      "1    2436\n",
      "8    1605\n",
      "6    1186\n",
      "4    1007\n",
      "0     814\n",
      "5     255\n",
      "7     243\n",
      "3      17\n",
      "Name: author_codes, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# This is the total number of paragraphs for each author and their corresponding author code\n",
    "print(paragraph_df['authors'].value_counts())\n",
    "print(paragraph_df.author_codes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for each author's number of paragraphs\n",
    "author_count_df['number_of_paragraphs'] = list(paragraph_df['authors'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>paragraph_length</th>\n",
       "      <th>number_of_paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edgeworth</td>\n",
       "      <td>3</td>\n",
       "      <td>3721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>melville</td>\n",
       "      <td>4</td>\n",
       "      <td>2636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>whitman</td>\n",
       "      <td>2</td>\n",
       "      <td>2436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chesterton</td>\n",
       "      <td>3</td>\n",
       "      <td>1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryant</td>\n",
       "      <td>2</td>\n",
       "      <td>1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>austen</td>\n",
       "      <td>4</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>carroll</td>\n",
       "      <td>2</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blake</td>\n",
       "      <td>2</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>burgess</td>\n",
       "      <td>4</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>milton</td>\n",
       "      <td>108</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      authors  paragraph_length  number_of_paragraphs\n",
       "0   edgeworth                 3                  3721\n",
       "1    melville                 4                  2636\n",
       "2     whitman                 2                  2436\n",
       "3  chesterton                 3                  1605\n",
       "4      bryant                 2                  1186\n",
       "5      austen                 4                  1007\n",
       "6     carroll                 2                   814\n",
       "7       blake                 2                   255\n",
       "8     burgess                 4                   243\n",
       "9      milton               108                    17"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10617f668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d853518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph for the number of paragraphs for each author\n",
    "y_pos = np.arange(len(author_count_df.number_of_paragraphs)) \n",
    "plt.barh(y_pos, author_count_df.number_of_paragraphs, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, author_count_df.authors)\n",
    "plt.ylabel('Authors')\n",
    "plt.title('Number of Paragraphs') \n",
    "plt.show()\n",
    "\n",
    "# Graph for the average length of paragraphs for each author\n",
    "y_pos = np.arange(len(author_count_df.paragraph_length)) \n",
    "plt.barh(y_pos, author_count_df.paragraph_length, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, author_count_df.authors)\n",
    "plt.ylabel('Authors')\n",
    "plt.title('Length of Paragraphs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the take aways from visualizing the authors and the paragraph dataset is that there is an important distinction between the number of paragraphs and the length of paragraphs. As you can see from the visualization above, John Milton has very few paragraphs compared to all the other authors but he makes up for it by having extremely long paragraphs. This will likely affect my clusters and models because Milton is the outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've created a dataframe and visualized some of the distribution for my dataset, I'm going to create features that I can use for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining my X and Y variables\n",
    "X = paragraph_df.paragraphs\n",
    "Y = paragraph_df.author_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 4179\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=6, # only use words that appear at six times\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "paragraph_tfidf=vectorizer.fit_transform(X)\n",
    "print(\"Number of features: %d\" % paragraph_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets. Reserving 25% of my corpus as a test set.\n",
    "X_train_tfidf, X_test_tfidf, Y_train, Y_test = train_test_split(paragraph_tfidf, \n",
    "                                                                Y,\n",
    "                                                                test_size=0.25,\n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of my large number of features, I want to reduce it to the most valuable components. I am going to use a loop and a Singular Value Decomposition(SVD) to get a better sense of how many components are needed to capture 50% of the variance for my dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Percent variance captured by all components: 33.324471111767906\n",
      "400\n",
      "Percent variance captured by all components: 46.77837565066583\n",
      "600\n",
      "Percent variance captured by all components: 56.161878622526885\n",
      "800\n",
      "Percent variance captured by all components: 63.499196088147805\n",
      "1000\n",
      "Percent variance captured by all components: 69.50808732691449\n"
     ]
    }
   ],
   "source": [
    "for i in range(200, 1100, 200):\n",
    "    svd= TruncatedSVD(i)\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "    # Run SVD on the training data, then project the training data.\n",
    "    X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "    variance_explained=svd.explained_variance_ratio_\n",
    "    total_variance = variance_explained.sum()\n",
    "    print(i)\n",
    "    print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 51.815549996354434\n"
     ]
    }
   ],
   "source": [
    "# From the code above, I'm going to reduce the feature space from 4179 to 500. \n",
    "svd= TruncatedSVD(500)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([67., 66., 58., 53., 41., 32., 25., 19., 13., 11., 11., 10.,  8.,\n",
       "         6.,  7.,  5.,  4.,  3.,  5.,  4.,  5.,  4.,  3.,  3.,  4.,  4.,\n",
       "         0.,  3.,  1.,  2.,  2.,  2.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  0.,  0.,  2.,  1.,  2.,  1.,  1.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
       " array([0.00041715, 0.00049552, 0.0005739 , 0.00065227, 0.00073064,\n",
       "        0.00080902, 0.00088739, 0.00096577, 0.00104414, 0.00112252,\n",
       "        0.00120089, 0.00127927, 0.00135764, 0.00143601, 0.00151439,\n",
       "        0.00159276, 0.00167114, 0.00174951, 0.00182789, 0.00190626,\n",
       "        0.00198464, 0.00206301, 0.00214138, 0.00221976, 0.00229813,\n",
       "        0.00237651, 0.00245488, 0.00253326, 0.00261163, 0.00269   ,\n",
       "        0.00276838, 0.00284675, 0.00292513, 0.0030035 , 0.00308188,\n",
       "        0.00316025, 0.00323863, 0.003317  , 0.00339537, 0.00347375,\n",
       "        0.00355212, 0.0036305 , 0.00370887, 0.00378725, 0.00386562,\n",
       "        0.003944  , 0.00402237, 0.00410074, 0.00417912, 0.00425749,\n",
       "        0.00433587, 0.00441424, 0.00449262, 0.00457099, 0.00464937,\n",
       "        0.00472774, 0.00480611, 0.00488449, 0.00496286, 0.00504124,\n",
       "        0.00511961, 0.00519799, 0.00527636, 0.00535474, 0.00543311,\n",
       "        0.00551148, 0.00558986, 0.00566823, 0.00574661, 0.00582498,\n",
       "        0.00590336, 0.00598173, 0.00606011, 0.00613848, 0.00621685,\n",
       "        0.00629523, 0.0063736 , 0.00645198, 0.00653035, 0.00660873,\n",
       "        0.0066871 , 0.00676548, 0.00684385, 0.00692222, 0.0070006 ,\n",
       "        0.00707897, 0.00715735, 0.00723572, 0.0073141 , 0.00739247,\n",
       "        0.00747085, 0.00754922, 0.00762759, 0.00770597, 0.00778434,\n",
       "        0.00786272, 0.00794109, 0.00801947, 0.00809784, 0.00817622,\n",
       "        0.00825459]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d9fd5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is a histogram showing that many of the components explain very little about the dataset and its variance. \n",
    "plt.hist(variance_explained, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering/Unsupervised Modeling\n",
    "\n",
    "To begin my clustering of authors, I used KMeans, MeanShift, and MiniBatchKMeans as my techniques. My impression is that none of the clustering methods performed very well. I tested out many different size of clusters but none of them were able to consistently group the authors. I chose to use 10 as my desired number of clusters given that I have 10 authors that are in my dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against authors:\n",
      "col_0           0    1    2     3    4    5    6   7    8    9\n",
      "author_codes                                                  \n",
      "0               3    0    3   307   27   30  178  10    3   19\n",
      "1               0    0   86   917   17   57    4   0   35  710\n",
      "2              64    0  109  1076  288   24   80  32   28  241\n",
      "3               0    0    0     5    0    0    0   0    0    6\n",
      "4              35    0   28   305   14   23   44  24  260   20\n",
      "5               0    0    3   116   19    5    4   4    3   50\n",
      "6               0    0   53   462  132   19  154  21    3   47\n",
      "7               0    0    3   107   43   10   10   2    3    4\n",
      "8              16  179   77   615   16   28  198  14    8   50\n",
      "9             103    0  154  1564   64  139  401  66  234  125\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data.\n",
    "X_norm = normalize(X_train_lsa)\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=10, random_state=42).fit_predict(X_norm)\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against authors:')\n",
    "print(pd.crosstab(Y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K means clustering does not start out showing any meaningful grouping of the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 1\n",
      "Comparing the assigned categories to the ones in the data:\n",
      "col_0            0\n",
      "author_codes      \n",
      "0              580\n",
      "1             1826\n",
      "2             1942\n",
      "3               11\n",
      "4              753\n",
      "5              204\n",
      "6              891\n",
      "7              182\n",
      "8             1201\n",
      "9             2850\n"
     ]
    }
   ],
   "source": [
    "# Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "# number based on an inspection of the distances among points in the data.\n",
    "bandwidth = estimate_bandwidth(X_train_lsa, quantile=0.2, n_samples=500)\n",
    "\n",
    "# Declare and fit the model.\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X_train_lsa)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluster centers.\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters.\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "print(pd.crosstab(Y_train,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Shift clustering performed very poorly because it decided that there was only one cluster in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means and mini batch k-means solutions:\n",
      "col_0           0    1   2   3    4    5    6    7    8     9\n",
      "author_codes                                                 \n",
      "0              33    3   0   0   40   23   11  171   16   283\n",
      "1              62    0  69   3  123  169  160    3   27  1210\n",
      "2              60   67  92   0  445   73   51   82   65  1007\n",
      "3               0    0   0   0    1    1    0    0    0     9\n",
      "4              26   40  17   0   35   27   20   51   43   494\n",
      "5               4    0   4   0   28   29   10    4    6   119\n",
      "6              23    0  34   0  175   35   37  145   46   396\n",
      "7              10    0   0   0   51    6    2   10    6    97\n",
      "8              33   19  88   0   55   15    5  270   36   680\n",
      "9             122  108  83  35  173  114  209  363  181  1462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Each batch will be made up of 200 data points.\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=10,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_train_lsa)\n",
    "\n",
    "# Add the new predicted cluster memberships to the data frame.\n",
    "predict_mini = minibatchkmeans.predict(X_train_lsa)\n",
    "\n",
    "# Check the MiniBatch model against the training set.\n",
    "print('Comparing k-means and mini batch k-means solutions:')\n",
    "print(pd.crosstab(Y_train, predict_mini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I move onto the Supervised modeling, I'm going to use my 25% reserve corpus test set on the MiniBatchKMeans model. I chose this model because it appeared to do a little better at separating some of the authors into separate groups. The test sample below shows that it does not appear to do any better at consistently separating the authors into their own clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing mini batch k-means solutions and the test group:\n",
      "col_0          0   1   2   3    4   5   6    7   8    9\n",
      "author_codes                                           \n",
      "0             18   0   4   0   17   7   7   61   8  112\n",
      "1             15   0  27   1   56  60  44    2  10  395\n",
      "2             24  27  40   0  169  35  14   19  15  351\n",
      "3              0   0   2   0    0   2   0    0   0    2\n",
      "4              7  12   6   0   17  10   7   13  16  166\n",
      "5              0   0   0   0    7  11  10    1   1   21\n",
      "6              2   0  12   0   67  12  14   41  17  130\n",
      "7              2   0   1   0   18   4   0    2   1   33\n",
      "8             18   2  34   0   24  10   1  104  13  198\n",
      "9             38  27  16  14   64  37  63  125  49  438\n"
     ]
    }
   ],
   "source": [
    "# Add the new predicted cluster memberships to the data frame.\n",
    "test_predict = minibatchkmeans.predict(X_test_lsa)\n",
    "\n",
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing mini batch k-means solutions and the test group:')\n",
    "print(pd.crosstab(Y_test, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Modeling\n",
    "\n",
    "Next, I am going to compare how several supervised models compare in their classification of the texts by author. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Training set score: 0.9450191570881226\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "rfc.fit(X_train_lsa,Y_train)\n",
    "print(rfc)\n",
    "print('Training set score:', rfc.score(X_train_lsa,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.32      0.39       234\n",
      "          1       0.50      0.64      0.56       610\n",
      "          2       0.56      0.54      0.55       694\n",
      "          3       0.00      0.00      0.00         6\n",
      "          4       0.60      0.47      0.53       254\n",
      "          5       0.12      0.02      0.03        51\n",
      "          6       0.42      0.29      0.34       295\n",
      "          7       0.83      0.31      0.45        61\n",
      "          8       0.65      0.45      0.54       404\n",
      "          9       0.51      0.68      0.58       871\n",
      "\n",
      "avg / total       0.53      0.53      0.52      3480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 74,  23,  23,   0,   9,   0,  14,   0,   9,  82],\n",
       "       [  9, 392,  84,   0,   4,   3,  13,   1,  19,  85],\n",
       "       [ 10, 138, 378,   0,  13,   0,  21,   1,  21, 112],\n",
       "       [  0,   5,   1,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  8,  19,  24,   0, 120,   1,   6,   1,   3,  72],\n",
       "       [  1,  25,  12,   0,   0,   1,   4,   0,   0,   8],\n",
       "       [ 13,  52,  36,   0,   7,   1,  86,   1,  18,  81],\n",
       "       [  1,   2,   7,   0,   0,   0,   8,  19,   2,  22],\n",
       "       [ 10,  43,  31,   0,   9,   1,  23,   0, 183, 104],\n",
       "       [ 18,  86,  84,   0,  37,   1,  30,   0,  25, 590]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Report:\")\n",
    "y_true, y_pred = Y_test, rfc.predict(X_test_lsa)\n",
    "print(classification_report(y_true, y_pred))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Each Cross Validated Accuracy score: \n",
      " [0.45285714 0.47701149 0.46264368 0.46695402 0.48121387]\n",
      "\n",
      "Overall Random Forest Classifier Accuracy: 0.47 (+/- 0.02)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RFC_score = cross_val_score(rfc, X_test_lsa, Y_test, cv=5)\n",
    "print('\\nEach Cross Validated Accuracy score: \\n', RFC_score)\n",
    "print(\"\\nOverall Random Forest Classifier Accuracy: %0.2f (+/- %0.2f)\\n\" % (RFC_score.mean(), RFC_score.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Random Forest Classifier I see that it performs poorly with a large amount of overfitting. My accuracy score is very bad a 0.47. The only thing that I'm happy to see in this model is that the confusion matrix is better than previous times when I was first working with my data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Training set score: 0.7524904214559387\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lf_model = lr.fit(X_train_lsa,Y_train)\n",
    "print(lf_model)\n",
    "print('Training set score:', lr.score(X_train_lsa,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.49      0.65       234\n",
      "          1       0.66      0.80      0.73       610\n",
      "          2       0.78      0.67      0.72       694\n",
      "          3       0.00      0.00      0.00         6\n",
      "          4       0.91      0.58      0.71       254\n",
      "          5       1.00      0.04      0.08        51\n",
      "          6       0.64      0.53      0.58       295\n",
      "          7       0.93      0.46      0.62        61\n",
      "          8       0.77      0.62      0.69       404\n",
      "          9       0.60      0.88      0.72       871\n",
      "\n",
      "avg / total       0.73      0.69      0.69      3480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[114,  12,  15,   0,   1,   0,  15,   0,   8,  69],\n",
       "       [  0, 488,  31,   0,   2,   0,   7,   0,   8,  74],\n",
       "       [  0,  82, 463,   0,   3,   0,  12,   0,  19, 115],\n",
       "       [  0,   4,   0,   0,   0,   0,   0,   0,   1,   1],\n",
       "       [  0,  10,  12,   0, 147,   0,   5,   0,   8,  72],\n",
       "       [  0,  33,   6,   0,   0,   2,   5,   0,   1,   4],\n",
       "       [  0,  30,  17,   0,   3,   0, 155,   2,  14,  74],\n",
       "       [  0,   3,   3,   0,   0,   0,   7,  28,   2,  18],\n",
       "       [  1,  30,  23,   0,   0,   0,  23,   0, 250,  77],\n",
       "       [  3,  43,  25,   0,   5,   0,  13,   0,  14, 768]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Report:\")\n",
    "y_true, y_pred = Y_test, lr.predict(X_test_lsa)\n",
    "print(classification_report(y_true, y_pred))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Each Cross Validated Accuracy score: \n",
      " [0.66571429 0.65948276 0.62787356 0.63218391 0.65317919]\n",
      "\n",
      "Overall Logistic Regression Accuracy: 0.65 (+/- 0.03)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_score = cross_val_score(lr, X_test_lsa, Y_test, cv=5)\n",
    "print('\\nEach Cross Validated Accuracy score: \\n', LR_score)\n",
    "print(\"\\nOverall Logistic Regression Accuracy: %0.2f (+/- %0.2f)\\n\" % (LR_score.mean(), LR_score.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model is a major improvement to all previous models. It has less overfitting and its end accuracy score is 0.65. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Training set score: 0.7683908045977011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "svc_model = svc.fit(X_train_lsa,Y_train)\n",
    "print(svc_model)\n",
    "print('Training set score:', svc_model.score(X_train_lsa,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.49      0.64       234\n",
      "          1       0.65      0.78      0.71       610\n",
      "          2       0.76      0.66      0.71       694\n",
      "          3       0.00      0.00      0.00         6\n",
      "          4       0.91      0.60      0.72       254\n",
      "          5       0.52      0.22      0.31        51\n",
      "          6       0.61      0.53      0.57       295\n",
      "          7       0.89      0.56      0.69        61\n",
      "          8       0.78      0.65      0.71       404\n",
      "          9       0.62      0.86      0.72       871\n",
      "\n",
      "avg / total       0.71      0.69      0.69      3480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[114,  17,  16,   0,   1,   1,  16,   0,   8,  61],\n",
       "       [  1, 475,  41,   0,   2,   4,  13,   0,   5,  69],\n",
       "       [  2,  88, 458,   0,   2,   1,  15,   1,  21, 106],\n",
       "       [  0,   4,   0,   0,   0,   0,   0,   0,   1,   1],\n",
       "       [  1,   9,  10,   0, 152,   0,   4,   0,   8,  70],\n",
       "       [  0,  29,   4,   0,   0,  11,   3,   1,   0,   3],\n",
       "       [  2,  29,  24,   0,   2,   3, 156,   1,  13,  65],\n",
       "       [  0,   4,   4,   0,   0,   1,   3,  34,   3,  12],\n",
       "       [  2,  28,  16,   0,   0,   0,  26,   1, 261,  70],\n",
       "       [  3,  45,  30,   0,   8,   0,  19,   0,  15, 751]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Report:\")\n",
    "y_true, y_pred = Y_test, svc.predict(X_test_lsa)\n",
    "print(classification_report(y_true, y_pred))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Each Cross Validated Accuracy score: \n",
      " [0.65142857 0.65517241 0.62212644 0.61925287 0.6416185 ]\n",
      "\n",
      "Overall Logistic Regression Accuracy: 0.64 (+/- 0.03)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVC_score = cross_val_score(svc, X_test_lsa, Y_test, cv=5)\n",
    "print('\\nEach Cross Validated Accuracy score: \\n', SVC_score)\n",
    "print(\"\\nOverall Logistic Regression Accuracy: %0.2f (+/- %0.2f)\\n\" % (SVC_score.mean(), SVC_score.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Support Vector Classifier had a similar performance to the logistic regression model. The only difference being that its accuracy is slightly lower at 0.64. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as I compare clustering and modeling with their ability to accurately classify the text, I would say that modeling easily out performed clustering. \n",
    "    \n",
    "The results from clustering were confusing and inconsistent. Clustering is somewhat of a black box that is difficulty to assess how or why it is connecting datapoints. I find that this makes it hard for a beginner to like myself to get a sense of how I can improve the clusters since so little about the clusters is easily communicated by the program. An advantage of unsupervised models and clustering in general is that they are good at exploring complicated dataset and making connections that supervised models can't make.\n",
    "    \n",
    "The supervised modeling that I used had much better and consistent outcomes. While there were some signs of overfitting, the models successfully separated out the majority of authors and their texts for both the logistic and support vector models. The advantages of modeling is that you can gain a much better perspective of how the data is organized and behaving. I would use modeling over clustering if I am trying to create a more explanatory model for the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
