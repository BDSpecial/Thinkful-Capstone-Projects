{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trauma Hospital Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is data that I scraped from the American College of Surgeons\n",
    "https://www.facs.org/search/trauma-centers?country=United%20States&n=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.facs.org/search/trauma-centers?country=United%20States&n=250',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for each in response.xpath('//div[@class=\"searchResults\"]/ul'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'hospital': each.xpath('li/h3/text()').extract_first(),\n",
    "                'address': each.xpath('li/text()').extract()[2],\n",
    "                'level': each.xpath('li[last()]/text()').extract_first()\n",
    "            }\n",
    "            \n",
    "        # Get the URL of the next page.\n",
    "        next_page = response.xpath('//a[@id=\"content_element_0_main_column_1_FullPagination_Next\"]/@href').extract_first()        \n",
    "        \n",
    "        # Recursively call the spider to run on the next page, if it exists.\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            # Request the next page and recursively parse it the same way we did above\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "            \n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'trauma_data.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospital Fall Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is data that I scraped from the profiles of thousands of hospitals, medical clinics, nursing homes and home health centers. http://www.hospital-data.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import itertools\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://www.hospital-data.com/index.html',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        state_urls = []\n",
    "        \n",
    "        # Iterate over every state element on the page.\n",
    "        for each in response.xpath('//div[@id=\"hospitals\"]/ul[@class=\"tab-list tab-list-long\"]'):\n",
    "            # Append all the state url to a list\n",
    "            state_urls.append(each.xpath('li/a/@href').extract())\n",
    "            \n",
    "        # Make the state urls into one list\n",
    "        state_urls = list(itertools.chain(*state_urls))\n",
    "        \n",
    "        each_state_url = []\n",
    "        # Get the entire url link for each state\n",
    "        for each in state_urls:\n",
    "            next_page = response.urljoin(each)\n",
    "            each_state_url.append(''+next_page)\n",
    "        # Call back each state's url into a new scraper. \n",
    "        for each in each_state_url:\n",
    "            request = scrapy.Request(each, callback=self.state_parse_info)\n",
    "            yield request\n",
    "\n",
    "    def state_parse_info(self, response):\n",
    "        \n",
    "        hospital_urls = []\n",
    "        # Loop over each medical facility in the state and make sure that it has one of these words\n",
    "        words_list = ['HOSPITAL', 'MEDICAL CENTER', 'HEALTH SYSTEM', 'UNIVERSITY']\n",
    "        for each in response.xpath('//tbody/tr/td'):\n",
    "            for one in each.xpath('a'):\n",
    "                if any(word in one.xpath('text()').extract_first() for word in words_list):\n",
    "                    # Append all the hospital urls to a list\n",
    "                    hospital_urls.append(one.xpath('@href').extract_first())\n",
    "          \n",
    "        each_hospital_url = []\n",
    "        # Get the entire url link for each hospital\n",
    "        for each in hospital_urls:\n",
    "            next_page = response.urljoin(each)\n",
    "            each_hospital_url.append(''+next_page)\n",
    "        # Call back each hospital's url into a new scraper. \n",
    "        for each in each_hospital_url:\n",
    "            request = scrapy.Request(each, callback=self.hospital_parse_info)\n",
    "            yield request\n",
    "            \n",
    "    def hospital_parse_info(self, response):\n",
    "        # Iterate over every 'hgraph' element on the page.\n",
    "        for each in response.xpath('//div[@class=\"hgraph\"]'):\n",
    "            for item in each.xpath('b/text()').extract():\n",
    "                # Only select the hospitals that have fall/injury data\n",
    "                if item == 'Falls and injuries':\n",
    "                    # Yield a dictionary with the values we want.\n",
    "                    yield {\n",
    "                        # This is the code to choose what we want to extract\n",
    "                        # Extracting the hospital name, hospital fall rate, and state fall rate. \n",
    "                        'hospital': response.xpath('//div[@class=\"container-fluid\"]//h1[@align=\"center\"]/text()').extract_first(),\n",
    "                        'hospital_falls': each.xpath('table//td/text()').extract()[0],\n",
    "                        'state_falls': each.xpath('table//td/text()').extract()[1]\n",
    "                    }\n",
    "            \n",
    "            \n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({ 'AUTOTHROTTLE_ENABLED': True, \n",
    "                          'HTTPCACHE_ENABLED': True, \n",
    "                          'ROBOTSTXT_OBEY': True,\n",
    "                          'DOWNLOAD_DELAY': 1,\n",
    "                          'USER_AGENT': 'Carley (clfletch91@gmail.com)',\n",
    "                          'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "                          'FEED_URI': 'hospital_data.json',  # Name our storage file.\n",
    "                          'LOG_ENABLED': False})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
